{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e796ad-bb84-427a-b33f-ca150cb4b880",
   "metadata": {},
   "source": [
    "This is a brief guide on how to set up a reinforcement learning (RL) environment that is compatible to the Gymnasium 1.0 interface. Gymnasium de facto defines the interface standard for RL environments and the library provides useful tools to work with RL environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737e3b4-c9e1-45eb-92ff-c150a2b27cbd",
   "metadata": {},
   "source": [
    "## Gymnasium\n",
    "\n",
    "Gymnasium is \"An API standard for reinforcement learning with a diverse collection of reference environments\" (https://gymnasium.farama.org/).\n",
    "\n",
    "It is a Python library that can be installed with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20bb2d1-930c-4cf5-9d82-39967b29c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (from gymnasium) (8.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (from gymnasium) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402152c2-3236-4697-9413-303f8eae8d3c",
   "metadata": {},
   "source": [
    "With the recent release of version 1.0.0 (2024/10/08) the API is stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cedcb-4e55-4b2d-b7f1-48bb59f3ebc0",
   "metadata": {},
   "source": [
    "## Components of Reinforcement Learning\n",
    "\n",
    "<img src=\"images/Agent-Env.png\" width=\"50%\"/>\n",
    "\n",
    "Source: Sutton, R.S., Barto A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press. http://incompleteideas.net/book/RLbook2020.pdf\n",
    "\n",
    "* state: should contain all information about the current state of the agent and the environment\n",
    "* action: should contain all information that determine the action of the agent in the environment\n",
    "* reward: tells the agent how well it performed based on the current state, its action, and the next state\n",
    "\n",
    "The distribution of the next state only depends on the current state and the action. The distribution of the reward only depends on the current state, action, and next state. That's the theory of Markov decision processes (MDPs). In practice, we often don't have the full state information. In this case we talk about observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c447e-8e07-44ea-aa51-188d5f4441a5",
   "metadata": {},
   "source": [
    "## Environment Interface\n",
    "\n",
    "The class `gymnasium.Env` ([link to documentation](https://gymnasium.farama.org/api/env/)) is the main class for implementing reinforcement learning environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0d726-e7f8-47dd-a563-41d1cecdfa75",
   "metadata": {},
   "source": [
    "### Constructor of an Environment (`__init__`)\n",
    "\n",
    "In the constructor, we have to define the **observation** and **action** space, which declare the general set of possible inputs (actions) and outputs (observations) of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eab24f-e8a9-4ebc-ba26-84be33d8c435",
   "metadata": {},
   "source": [
    "### Gymnasium Spaces Interface\n",
    "\n",
    "Spaces describe mathematical sets and are used in Gym to specify valid **actions** and **observations**.\n",
    "\n",
    "Every Gym environment must have the attributes `action_space` and `observation_space`. If, for instance, three possible actions (0,1,2) can be performed in your environment and observations are vectors in the two-dimensional unit cube, the environment code may contain the following two lines:\n",
    "\n",
    "```python\n",
    "self.action_space = spaces.Discrete(3)\n",
    "self.observation_space = spaces.Box(0, 1, shape=(2,))\n",
    "```\n",
    "\n",
    "We usually don't need to implement our own spaces, since Gymnasium provides various implementations of spaces:\n",
    "\n",
    "* `gymnasium.spaces.Box` ([link to documentation](https://gymnasium.farama.org/api/spaces/fundamental/)): A bounded or unbounded box in $\\mathbb{R}^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of $[a, b]$, $(-\\infty, b]$, $[a, \\infty)$, or $(-\\infty, \\infty)$.\n",
    "* `gymnasium.spaces.Discrete` ([link to documentation](https://gymnasium.farama.org/api/spaces/fundamental/#gymnasium.spaces.Discrete)): A space consisting of finitely many elements. This class represents a finite subset of integers, more specifically a set of the form $\\{ a, a+1, \\dots, a+n-1 \\}$.\n",
    "* see [documentation](https://gymnasium.farama.org/api/spaces/#fundamental-spaces) for more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a288062-4434-4536-a008-e9423745042f",
   "metadata": {},
   "source": [
    "### Attributes of Environment\n",
    "\n",
    "An environment usually contains the following attributes:\n",
    "\n",
    "* `Env.action_space: spaces.Space[ActType]` - The Space object corresponding to valid actions, all valid actions should be contained with the space.\n",
    "* `Env.observation_space: spaces.Space[ObsType]` - The Space object corresponding to valid observations, all valid observations should be contained with the space.\n",
    "* `Env.metadata: dict[str, Any] = {'render_modes': []}` - The metadata of the environment containing rendering modes, rendering fps, etc.\n",
    "* `Env.render_mode: str | None = None` - The render mode of the environment determined at initialisation.\n",
    "* `Env.spec: EnvSpec | None = None` - The EnvSpec of the environment normally set during `gymnasium.make()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3f1e6-2d7c-475b-a900-2ffdb3b7a288",
   "metadata": {},
   "source": [
    "### Minimal Interface\n",
    "\n",
    "The minimum of functions that need to be implemented for a new environment are\n",
    "\n",
    "* `Env.step(action: ActType) -> tuple[ObsType, float, bool, bool, dict[str, Any]]`\n",
    "    * Run one timestep of the environment's dynamics using the agent actions.\n",
    "    * Parameters\n",
    "        * **action** (ActType) - an action provided by the agent to update the environment state. ActType is usually a NumPy array with `ndim == 1`.\n",
    "    * Returns\n",
    "        * **observation** (ObsType) - An element of the environment’s observation_space as the next observation due to the agent actions. ObsType is usually a NumPy array with `ndim == 1`.\n",
    "        * **reward** (float) - The reward as a result of taking the action.\n",
    "        * **terminated** (bool) - Whether the agent reaches the terminal state (as defined under the MDP of the task) which can be positive or negative. An example is reaching the goal state or moving into the lava from the Sutton and Barto Gridworld. If true, the user needs to call `reset()`.\n",
    "        * **truncated** (bool) - Whether the truncation condition outside the scope of the MDP is satisfied. Typically, this is a timelimit, but could also be used to indicate an agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached. If true, the user needs to call `reset()`.\n",
    "        * **info** (dict) - Contains auxiliary diagnostic information (helpful for debugging, learning, and logging). This might, for instance, contain: metrics that describe the agent's performance state, variables that are hidden from observations, or individual reward terms that are combined to produce the total reward.\n",
    "* `Env.reset(seed: int = None, options: dict[str, Any] = None) → tuple[ObsType, dict[str, Any]]`\n",
    "    * Resets the environment to an initial internal state, returning an initial observation and info.\n",
    "    * Parameters\n",
    "        * **seed** (optional int) - The seed that is used to initialize the environment’s PRNG (np_random) and the read-only attribute np_random_seed. If the environment does not already have a PRNG and seed=None (the default option) is passed, a seed will be chosen from some source of entropy (e.g., timestamp or /dev/urandom). However, if the environment already has a PRNG and seed=None is passed, the PRNG will not be reset and the env's np_random_seed will not be altered. If you pass an integer, the PRNG will be reset even if it already exists. Usually, you want to pass an integer right after the environment has been initialized and then never again.\n",
    "        * **options** (optional dict) - Additional information to specify how the environment is reset (optional, depending on the specific environment)\n",
    "    * Returns\n",
    "        * **observation** (ObsType) - Observation of the initial state. This will be an element of observation_space (typically a NumPy array) and is analogous to the observation returned by `step()`.\n",
    "        * **info** (dictionary) - This dictionary contains auxiliary information complementing observation. It should be analogous to the info returned by `step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b7cf2-477c-442a-8af7-f2533226ea5d",
   "metadata": {},
   "source": [
    "### Additional Interface\n",
    "\n",
    "In addition, the following functions *should* be implemented.\n",
    "\n",
    "* `Env.render() → RenderFrame | list[RenderFrame] | None`\n",
    "    * Compute the render frames as specified by render_mode during the initialization of the environment.\n",
    "    * By convention, if the render_mode is:\n",
    "        * `None` (default): no render is computed.\n",
    "        * `\"human\"`: The environment is continuously rendered in the current display or terminal, usually for human consumption. This rendering should occur during `step()` and `render()` doesn’t need to be called. Returns `None`.\n",
    "        * `\"rgb_array\"`: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape `(x, y, 3)` representing RGB values for an x-by-y pixel image.\n",
    "        * ... there are more conventions defined [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.render).\n",
    "* `Env.close()`\n",
    "    * After the user has finished using the environment, close contains the code necessary to \"clean up\" the environment. This is critical for closing rendering windows, database or HTTP connections. Calling close on an already closed environment has no effect and won’t raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394753a-b5d4-475e-8d92-78ade7dd9175",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5c7863-d082-4b84-9f64-9c295cec2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class OmnidirectionalRobot2DEnv(gym.Env):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # just to show that we can pass arguments to the constructor\n",
    "            robot_name=\"Otto\"\n",
    "    ):\n",
    "        self.robot_name = robot_name\n",
    "        \n",
    "        self._robot_location = np.array([0.5, 0.5])\n",
    "        self._goal_location = np.array([0.2, 0.1])\n",
    "        self._n_steps = 0\n",
    "        \n",
    "        # observation_space is a public interface\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([0.0, 0.0, 0.0, 0.0]),\n",
    "            high=np.array([1.0, 1.0, 0.5, 0.5]),\n",
    "            shape=(4,),\n",
    "            dtype=float,\n",
    "            seed=self.np_random\n",
    "        )\n",
    "\n",
    "        # action_space is a public interface\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([-0.1, -0.1]),\n",
    "            high=np.array([0.1, 0.1]),\n",
    "            shape=(2,),\n",
    "            dtype=float,\n",
    "            seed=self.np_random\n",
    "        )\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # this is necessary to properly initialize the random number generator\n",
    "        super().reset(seed=seed)\n",
    "        self.observation_space.seed(seed)\n",
    "        self.action_space.seed(seed)\n",
    "        \n",
    "        obs = self.observation_space.sample()\n",
    "        self._robot_location = obs[:2]\n",
    "        self._goal_location = obs[2:]\n",
    "        \n",
    "        self._n_steps = 0\n",
    "        \n",
    "        info = {  # can be empty or contain useful information\n",
    "            \"goal_location\": self._goal_location\n",
    "        }\n",
    "        \n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self._n_steps += 1\n",
    "        \n",
    "        # this is very important: map action to a valid action\n",
    "        # in this case we just clip the action to the action space\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        \n",
    "        # random motion of goal, use the internal random number generator\n",
    "        # that uses the numpy interface:\n",
    "        # https://numpy.org/doc/stable/reference/random/generator.html\n",
    "        self._goal_location += 0.05 * self.np_random.random(2)\n",
    "        \n",
    "        self._robot_location += action\n",
    "        \n",
    "        obs = np.hstack((self._robot_location, self._goal_location))\n",
    "        obs = np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "        self._robot_location = obs[:2]\n",
    "        self._goal_location = obs[2:]\n",
    "        \n",
    "        distance = np.linalg.norm(self._goal_location - self._robot_location)\n",
    "        \n",
    "        # Episode successfully terminated?\n",
    "        terminated = distance < 0.1\n",
    "        # Truncation with the number of timesteps is not necessary,\n",
    "        # we have an environment wrapper in Gymnasium that can do\n",
    "        # this. It is more important to check limits in the state space,\n",
    "        # e.g., did the robot crash into an obstacle? Is the temperature\n",
    "        # to high? Did the robot explode?\n",
    "        truncated = self._n_steps >= 100\n",
    "        reward = -distance\n",
    "        info = {  # same as in reset()\n",
    "            \"goal_location\": self._goal_location\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    #def render(self):\n",
    "    # Implementation of a render function is highly recommended for debug purposes.\n",
    "    # Rendering can produce a text (mode: 'ansi') or an image (mode: 'rgb_array').\n",
    "    \n",
    "    def close(self):\n",
    "        pass  # shut down simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00be946-d765-40ae-b5ea-cd9cabd6aeee",
   "metadata": {},
   "source": [
    "## Check the Environment\n",
    "\n",
    "Gymnasium provides the function `gymnasium.utils.env_checker.check_env(env: Env)` to verify environments ([link to documentation](https://gymnasium.farama.org/api/utils/#environment-checking))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634e7fc3-0001-42df-a55e-6fecd3e73f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfki.uni-bremen.de/afabisch/anaconda3/lib/python3.9/site-packages/gymnasium/utils/env_checker.py:434: UserWarning: \u001b[33mWARN: Not able to test alternative render modes due to the environment not having a spec. Try instantiating the environment through `gymnasium.make`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.env_checker import check_env\n",
    "check_env(OmnidirectionalRobot2DEnv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c6faf-278e-420d-8c99-412f2109ae41",
   "metadata": {},
   "source": [
    "## Registering an Environment\n",
    "\n",
    "Gymnasium environments are usually created with `gymnasium.make_env(id)`. Before this, external environments have to be registered, so that Gymnasium knows how to create them. You can do this with\n",
    "\n",
    "```python\n",
    "gymnasium.register(\n",
    "    # The environment id (name).\n",
    "    id: str,\n",
    "    # The entry point for creating the environment.\n",
    "    entry_point: EnvCreator | str | None = None,\n",
    "    # The reward threshold considered for an agent to have learnt the environment.\n",
    "    reward_threshold: float | None = None,\n",
    "    # If the environment is nondeterministic, i.e., even with knowledge of the initial seed and all actions, the same state cannot be reached.\n",
    "    nondeterministic: bool = False,\n",
    "    # The maximum number of episodes steps before truncation.\n",
    "    max_episode_steps: int | None = None,\n",
    "    # Arbitrary keyword arguments which are passed to the environment constructor on initialisation.\n",
    "    kwargs: dict | None = None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf81e3d-8845-4993-b18a-65fb663e2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    \"OmnidirectionalRobot2DEnv-v0\",\n",
    "    entry_point=OmnidirectionalRobot2DEnv,\n",
    "    reward_threshold=1.0,\n",
    "    nondeterministic=False,\n",
    "    max_episode_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62486f3-4c1c-4cf0-8b94-8976955a5204",
   "metadata": {},
   "source": [
    "After registering the environment, we can see it when we print it in the last group of the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39921dd7-983b-48c6-aa6e-5749dd75929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== classic_control =====\n",
      "Acrobot-v1                CartPole-v0               CartPole-v1\n",
      "MountainCar-v0            MountainCarContinuous-v0  Pendulum-v1\n",
      "===== phys2d =====\n",
      "phys2d/CartPole-v0        phys2d/CartPole-v1        phys2d/Pendulum-v0\n",
      "===== box2d =====\n",
      "BipedalWalker-v3          BipedalWalkerHardcore-v3  CarRacing-v3\n",
      "LunarLander-v3            LunarLanderContinuous-v3\n",
      "===== toy_text =====\n",
      "Blackjack-v1              CliffWalking-v0           FrozenLake-v1\n",
      "FrozenLake8x8-v1          Taxi-v3\n",
      "===== tabular =====\n",
      "tabular/Blackjack-v0      tabular/CliffWalking-v0\n",
      "===== mujoco =====\n",
      "Ant-v2                    Ant-v3                    Ant-v4\n",
      "Ant-v5                    HalfCheetah-v2            HalfCheetah-v3\n",
      "HalfCheetah-v4            HalfCheetah-v5            Hopper-v2\n",
      "Hopper-v3                 Hopper-v4                 Hopper-v5\n",
      "Humanoid-v2               Humanoid-v3               Humanoid-v4\n",
      "Humanoid-v5               HumanoidStandup-v2        HumanoidStandup-v4\n",
      "HumanoidStandup-v5        InvertedDoublePendulum-v2 InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5 InvertedPendulum-v2       InvertedPendulum-v4\n",
      "InvertedPendulum-v5       Pusher-v2                 Pusher-v4\n",
      "Pusher-v5                 Reacher-v2                Reacher-v4\n",
      "Reacher-v5                Swimmer-v2                Swimmer-v3\n",
      "Swimmer-v4                Swimmer-v5                Walker2d-v2\n",
      "Walker2d-v3               Walker2d-v4               Walker2d-v5\n",
      "===== None =====\n",
      "GymV21Environment-v0      GymV26Environment-v0      OmnidirectionalRobot2DEnv-v0\n"
     ]
    }
   ],
   "source": [
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4a1b4-5a28-46f3-ae73-8ce8c23d1308",
   "metadata": {},
   "source": [
    "Then we can create a new environment with `gymnasium.make()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d0dc05-5b96-4265-8c57-672e80704ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"OmnidirectionalRobot2DEnv-v0\",\n",
    "    max_episode_steps=200,\n",
    "    robot_name=\"Peter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbff0151-9cc9-46d3-a281-aa50d7327dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peter'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gymnasium puts many wrappers around the environment,\n",
    "# but the robot name is eventually passed to the constructor:\n",
    "env.unwrapped.robot_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711d091-2d6f-4ea4-b014-f2e9250ed228",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "* Gymnasium: Create a Custom Environment, https://gymnasium.farama.org/introduction/create_custom_env/\n",
    "* Gymnasium: Make your own custom environment, https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
